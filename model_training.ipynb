{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Pneumonia Detection from Chest X-Rays\n",
    "\n",
    "This notebook contains the complete code to train a deep learning model for classifying chest X-ray images as either **Normal** or **Pneumonia**. \n",
    "\n",
    "We will leverage **transfer learning** with a pre-trained **ResNet18** model and address the common challenge of **class imbalance** in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "First, we import all the necessary libraries. This includes:\n",
    "- `torch` and `torch.nn` for building the neural network.\n",
    "- `torch.optim` for the optimization algorithm.\n",
    "- `torchvision` for data transformations, datasets, and pre-trained models.\n",
    "- `safetensors` for securely saving our trained model.\n",
    "- `os` and `time` for utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from safetensors.torch import save_file\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configuration\n",
    "\n",
    "Here, we define the key parameters for our training process. This makes it easy to adjust settings like batch size or the number of epochs without searching through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'chest_xray'\n",
    "MODEL_SAVE_PATH = 'pneumonia_model_resnet.safetensors'\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Augmentation and Transformation\n",
    "\n",
    "Data augmentation is crucial for preventing overfitting and helping the model generalize better. For our training data, we apply random flips and rotations. For both training and validation data, we resize the images and normalize them with the standard ImageNet mean and standard deviation, as expected by the pre-trained ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load Data and Create DataLoaders\n",
    "\n",
    "We use `ImageFolder` to load our dataset, which automatically labels images based on their parent folder's name. We also define a helper function `is_valid_image` to skip any hidden or corrupted files that might be present in the dataset.\n",
    "\n",
    "The `DataLoader` then wraps the dataset, providing an efficient way to iterate over data in batches. We set `num_workers=2` to load data in parallel, which significantly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(path):\n",
    "    if os.path.basename(path).startswith('.'): return False\n",
    "    return path.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(\n",
    "                        os.path.join(DATA_DIR, x),\n",
    "                        transform=data_transforms[x],\n",
    "                        is_valid_file=is_valid_image\n",
    "                    )\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: DataLoader(\n",
    "                    image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, num_workers=2\n",
    "                )\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Training data size: {dataset_sizes['train']}\")\n",
    "print(f\"Validation data size: {dataset_sizes['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Address Class Imbalance with Weighted Loss\n",
    "\n",
    "The dataset has significantly more 'PNEUMONIA' images than 'NORMAL' images. If we don't account for this, the model might become biased and simply learn to predict the majority class. \n",
    "\n",
    "To fix this, we calculate class weights that are inversely proportional to the number of samples in each class. We then pass these weights to our loss function (`CrossEntropyLoss`), which will penalize mistakes on the minority class ('NORMAL') more heavily, forcing the model to pay more attention to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count = len(os.listdir(os.path.join(DATA_DIR, 'train', 'NORMAL')))\n",
    "pneumonia_count = len(os.listdir(os.path.join(DATA_DIR, 'train', 'PNEUMONIA')))\n",
    "total_count = normal_count + pneumonia_count\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    total_count / (2.0 * normal_count),\n",
    "    total_count / (2.0 * pneumonia_count)\n",
    "]).to(device)\n",
    "\n",
    "print(f\"Normal images: {normal_count}, Pneumonia images: {pneumonia_count}\")\n",
    "print(f\"Calculated weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Load a Pre-trained Model (Transfer Learning)\n",
    "\n",
    "Instead of training a model from scratch, we use **transfer learning**. We load a **ResNet18** model that has already been pre-trained on the massive ImageNet dataset. This model already knows how to recognize fundamental features like edges, shapes, and textures.\n",
    "\n",
    "1.  **Freeze Layers:** We freeze all the existing layers of the model so their weights won't be updated during training.\n",
    "2.  **Replace Final Layer:** We replace the model's final fully connected layer (`fc`) with a new one tailored to our specific task (classifying 2 classes: Normal vs. Pneumonia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"ResNet18 model loaded and final layer replaced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define Loss Function and Optimizer\n",
    "\n",
    "- **Loss Function:** We use `CrossEntropyLoss` and pass in our calculated `class_weights`.\n",
    "- **Optimizer:** We use the `Adam` optimizer. Crucially, we only pass `model.fc.parameters()` to it. This ensures that the optimizer will **only update the weights of our new, final layer**, leaving the rest of the pre-trained model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: The Training Loop\n",
    "\n",
    "This is the core of the training process. We iterate through our data for a specified number of epochs. In each epoch:\n",
    "\n",
    "1.  **Training Phase:** We set the model to `train()` mode, process the training data, calculate the loss, and update the model's weights using backpropagation.\n",
    "2.  **Validation Phase:** We set the model to `eval()` mode, process the validation data, and calculate the accuracy. This gives us an unbiased measure of how well our model is performing on data it hasn't seen before.\n",
    "3.  **Save Best Model:** If the validation accuracy in the current epoch is the best we've seen so far, we save the model's state dictionary to a file using `safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting training with ResNet18...\")\n",
    "start_time = time.time()\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            save_file(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"New best model saved to {MODEL_SAVE_PATH} with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best val Acc: {best_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
